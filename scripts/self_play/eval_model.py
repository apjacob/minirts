# Copyright (c) Facebook, Inc. and its affiliates.# All rights reserved.## This source code is licensed under the license found in the# LICENSE file in the root directory of this source tree.#import argparseimport osimport sysimport pprintfrom set_path import append_sys_pathappend_sys_path()import torchimport randomimport tubefrom agent import Agentfrom pytube import DataChannelManagerfrom common_utils import StateActionBufferfrom torch.utils.tensorboard import SummaryWriterimport minirtsimport numpy as npimport picklefrom collections import defaultdictimport torch.optim as optimfrom rnn_coach import ConvRnnCoachfrom onehot_coach import ConvOneHotCoachfrom rnn_generator import RnnGeneratorfrom itertools import groupbyfrom executor_wrapper import ExecutorWrapperfrom executor import Executorfrom common_utils import to_device, ResultStat, Loggerfrom best_models import best_executors, best_coachesfrom tqdm import tqdmfrom game import *import jsonfrom agent import run_evalimport statisticsdef parse_args():    parser = argparse.ArgumentParser(description='human coach')    parser.add_argument('--seed', type=int, default=1)    parser.add_argument('--num_thread', type=int, default=1)    parser.add_argument('--num_iter', type=int, default=10)    parser.add_argument('--train_batch_size', type=int, default=32)    parser.add_argument('--tag', type=str, default='')    parser.add_argument('--sampling_freq', type=float, default=0.4)    parser.add_argument('--sp_factor', type=int, default=2)    parser.add_argument('--eval_factor', type=int, default=10)    parser.add_argument('--max_table_size', type=int, default=100)    parser.add_argument('--game_per_thread', type=int, default=1)    parser.add_argument('--gpu', type=int, default=0)    root = os.path.dirname(        os.path.dirname(os.path.dirname(os.path.abspath(__file__))))    default_lua = os.path.join(root, 'game/game_MC/lua')    parser.add_argument('--lua_files', type=str, default=default_lua)    # ai1 option    parser.add_argument('--frame_skip', type=int, default=50)    parser.add_argument('--fow', type=int, default=1)    parser.add_argument('--use_moving_avg', type=int, default=1)    parser.add_argument('--moving_avg_decay', type=float, default=0.98)    parser.add_argument('--num_resource_bins', type=int, default=11)    parser.add_argument('--resource_bin_size', type=int, default=50)    parser.add_argument('--max_num_units', type=int, default=50)    parser.add_argument('--num_prev_cmds', type=int, default=25)    # TOOD: add max instruction span    parser.add_argument('--max_raw_chars', type=int, default=200)    parser.add_argument('--verbose', action='store_true')    parser.add_argument('--inst_mode', type=str, default='full') # can be full/good/better    # game option    parser.add_argument('--max_tick', type=int, default=int(2e4))    parser.add_argument('--no_terrain', action='store_true')    parser.add_argument('--resource', type=int, default=500)    parser.add_argument('--resource_dist', type=int, default=4)    parser.add_argument('--fair', type=int, default=0)    parser.add_argument('--save_replay_freq', type=int, default=0)    parser.add_argument('--save_replay_per_games', type=int, default=1)    parser.add_argument('--save_dir', type=str, default='matches2/dev')    parser.add_argument('--save_folder', type=str, default='/home/ubuntu/minirts/scripts/self_play/save')    parser.add_argument('--load_file', type=str, default='/home/ubuntu/minirts/scripts/self_play/save/')    parser.add_argument('--reload', type=int, default=0)    parser.add_argument('--tb_log', type=int, default=1)    # full vision    parser.add_argument('--cheat', type=int, default=0)    parser.add_argument('--coach1', type=str, default='')    parser.add_argument('--executor1', type=str, default='')    parser.add_argument('--coach2', type=str, default='')    parser.add_argument('--executor2', type=str, default='')    parser.add_argument('--lr', type=float, default=1e-2)    parser.add_argument('--beta1', type=float, default=0.9)    parser.add_argument('--beta2', type=float, default=0.999)    parser.add_argument('--grad_clip', type=float, default=0.5)    parser.add_argument('--pg', type=str, default='vanilla')    parser.add_argument('--ppo_eps', type=float, default=0.2)    parser.add_argument('--ppo_epochs', type=int, default=3)    args = parser.parse_args()    return argsdef self_play():    global device    args = parse_args()    print('args:')    pprint.pprint(vars(args))    os.environ['LUA_PATH'] = os.path.join(args.lua_files, '?.lua')    print('lua path:', os.environ['LUA_PATH'])    if not os.path.exists(args.save_dir):        os.makedirs(args.save_dir)    if args.reload:        print("Reloading coach model.... ")        args.coach1 = args.load_file        log_name = "test_replay_adaptive_coach_c1_type={}__e1_type={}_lr={}__num_iter={}__num_thread={}__sp_factor={}__pg={}_{}_{}".\                    format(os.path.basename(args.coach1).replace(".pt", ""),                   args.executor1, args.lr, args.num_iter, args.num_thread, args.sp_factor, args.pg,                           args.tag, random.randint(1111, 9999))        writer = SummaryWriter(            comment=log_name)    else:        log_name = "test_sp_adaptive_coach_c1_type={}__e1_type={}_lr={}__num_iter={}__num_thread={}__sp_factor={}__pg={}_{}_{}".\            format(args.coach1, args.executor1, args.lr, args.num_iter, args.num_thread, args.sp_factor,                   args.pg, args.tag, random.randint(1111, 9999))        writer = SummaryWriter(            comment=log_name)    ## Coach, Executor    logger_path = os.path.join(args.save_dir, 'train.log')    sys.stdout = Logger(logger_path)    test1 = {'rnn500-rnn': ('rnn500', 'rnn')}    test2 = {'rnn50-rnn': ('rnn50', 'rnn'),             'rnn250-rnn': ('rnn250', 'rnn'),             'rnn500-rnn': ('rnn500', 'rnn')             # 'bow500-bow': ('bow50', 'bow'),             # 'onehot500-onehot': ('onehot500', 'onehot'),             }    full_result = {}    game_count = 0    for a1, a1_tup in test1.items():        for a2, a2_tup in test2.items():            print("########################################################")            print("Playing {} vs {}:  ".format(a1, a2))            if a1 == 'rl':                args.coach1 = a1_tup[0]                args.executor1 = best_executors[a1_tup[1]]            else:                args.coach1 = best_coaches[a1_tup[0]]                args.executor1 = best_executors[a1_tup[1]]            if a2 == 'rl':                args.coach2 = a2_tup[0]                args.executor2 = best_executors[a2_tup[1]]            else:                args.coach2 = best_coaches[a2_tup[0]]                args.executor2 = best_executors[a2_tup[1]]            device = torch.device('cuda:%d' % args.gpu)            sp_agent = Agent(coach=args.coach1, executor=args.executor1, device=device, args=args, writer=writer, trainable=True, pg=args.pg)            bc_agent = Agent(coach=args.coach2, executor=args.executor2, device=device, args=args, writer=writer, trainable=False)            print("Progress: ")            a1_win = []            a1_loss = []            a1_draw = []            num_games = 150            for index in range(args.num_iter):                game_count+=1                result1, result2 = sp_agent.eval_model(index, bc_agent, num_games)                a1_win.append(result1.win/num_games)                a1_loss.append(result2.win/num_games)                a1_draw.append((num_games - (result1.win + result2.win))/num_games)            print("Agent 1 Win: {} +/- {}".format(statistics.mean(a1_win), statistics.stdev(a1_win)))            print("Agent 1 Loss: {} +/- {}".format(statistics.mean(a1_loss), statistics.stdev(a1_loss)))            print("Agent 1 Draw: {} +/- {}".format(statistics.mean(a1_draw), statistics.stdev(a1_draw)))            full_result["{}-vs-{}".format(a1, a2)] = (statistics.mean(a1_win), statistics.mean(a1_loss), statistics.mean(a1_draw))            print("########################################################")        with open('gameplay_dump_{}.json'.format(game_count), 'w') as fp:            json.dump(full_result, fp)    print(full_result)    writer.close()if __name__ == '__main__':    self_play()