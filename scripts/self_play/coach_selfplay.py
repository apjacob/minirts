# Copyright (c) Facebook, Inc. and its affiliates.# All rights reserved.## This source code is licensed under the license found in the# LICENSE file in the root directory of this source tree.#import argparseimport osimport sysimport pprintfrom set_path import append_sys_pathappend_sys_path()import torchimport randomimport tubefrom pytube import DataChannelManagerfrom torch.utils.tensorboard import SummaryWriterimport minirtsimport numpy as npimport picklefrom collections import defaultdictimport torch.optim as optimfrom rnn_coach import ConvRnnCoachfrom onehot_coach import ConvOneHotCoachfrom rnn_generator import RnnGeneratorfrom itertools import groupbyfrom executor_wrapper import ExecutorWrapperfrom executor import Executorfrom common_utils import to_device, ResultStat, Loggerfrom best_models import best_executors, best_coachesfrom tqdm import tqdmreward_tuple = [("win", 1), ("loss", -1)]def init_games(num_games, ai1_option, ai2_option, game_option, *, act_name='act'):    # print('ai1 option:')    # print(ai1_option.info())    # print('ai2 option:')    # print(ai2_option.info())    # print('game option:')    # print(game_option.info())    batchsize = min(32, max(num_games // 2, 1))    act1_dc = tube.DataChannel(act_name+'1', batchsize, 1)    act2_dc = tube.DataChannel(act_name+'2', batchsize, 1)    context = tube.Context()    idx2utype = [        minirts.UnitType.SPEARMAN,        minirts.UnitType.SWORDMAN,        minirts.UnitType.CAVALRY,        minirts.UnitType.DRAGON,        minirts.UnitType.ARCHER,    ]    if game_option.seed == 777:        print("Using random seeds...")        seed = random.randint(1, 123456)    else:        seed = game_option.seed    for i in range(num_games):        g_option = minirts.RTSGameOption(game_option)        g_option.seed = seed + i        g_option.game_id = str(i)        if game_option.save_replay_prefix:            g_option.save_replay_prefix = game_option.save_replay_prefix + "_0_" + str(i)        g = minirts.RTSGame(g_option)        bot1 = minirts.CheatExecutorAI(ai1_option, 0, None, act1_dc)        bot2 = minirts.CheatExecutorAI(ai2_option, 0, None, act2_dc)        # utype = idx2utype[i % len(idx2utype)]        # bot2 = minirts.MediumAI(ai2_option, 0, None, utype, False)        # p1dict[i] = []        # p2dict[i] = []        g.add_bot(bot1)        g.add_bot(bot2)        context.push_env_thread(g)    return context, act1_dc, act2_dcdef parse_args():    parser = argparse.ArgumentParser(description='human coach')    parser.add_argument('--seed', type=int, default=1)    parser.add_argument('--num_thread', type=int, default=1)    parser.add_argument('--num_iter', type=int, default=10)    parser.add_argument('--sampling_freq', type=float, default=0.4)    parser.add_argument('--sp_factor', type=int, default=2)    parser.add_argument('--eval_factor', type=int, default=10)    parser.add_argument('--game_per_thread', type=int, default=1)    parser.add_argument('--gpu', type=int, default=0)    root = os.path.dirname(        os.path.dirname(os.path.dirname(os.path.abspath(__file__))))    default_lua = os.path.join(root, 'game/game_MC/lua')    parser.add_argument('--lua_files', type=str, default=default_lua)    # ai1 option    parser.add_argument('--frame_skip', type=int, default=50)    parser.add_argument('--fow', type=int, default=1)    parser.add_argument('--use_moving_avg', type=int, default=1)    parser.add_argument('--moving_avg_decay', type=float, default=0.98)    parser.add_argument('--num_resource_bins', type=int, default=11)    parser.add_argument('--resource_bin_size', type=int, default=50)    parser.add_argument('--max_num_units', type=int, default=50)    parser.add_argument('--num_prev_cmds', type=int, default=25)    # TOOD: add max instruction span    parser.add_argument('--max_raw_chars', type=int, default=200)    parser.add_argument('--verbose', action='store_true')    parser.add_argument('--inst_mode', type=str, default='full') # can be full/good/better    # game option    parser.add_argument('--max_tick', type=int, default=int(2e4))    parser.add_argument('--no_terrain', action='store_true')    parser.add_argument('--resource', type=int, default=500)    parser.add_argument('--resource_dist', type=int, default=4)    parser.add_argument('--fair', type=int, default=0)    parser.add_argument('--save_replay_freq', type=int, default=0)    parser.add_argument('--save_replay_per_games', type=int, default=1)    parser.add_argument('--save_dir', type=str, default='matches2/dev')    parser.add_argument('--save_folder', type=str, default='/home/ubuntu/minirts/scripts/self_play/save')    parser.add_argument('--load_file', type=str, default='/home/ubuntu/minirts/scripts/self_play/save/')    parser.add_argument('--reload', type=int, default=0)    # full vision    parser.add_argument('--cheat', type=int, default=0)    parser.add_argument('--coach1', type=str, default='')    parser.add_argument('--executor1', type=str, default='')    parser.add_argument('--coach2', type=str, default='')    parser.add_argument('--executor2', type=str, default='')    parser.add_argument('--lr', type=float, default=1e-2)    parser.add_argument('--beta1', type=float, default=0.9)    parser.add_argument('--beta2', type=float, default=0.999)    parser.add_argument('--grad_clip', type=float, default=0.5)    args = parser.parse_args()    return argsdef get_game_option(args):    game_option = minirts.RTSGameOption()    game_option.seed = args.seed    game_option.max_tick = args.max_tick    game_option.no_terrain = args.no_terrain    game_option.resource = args.resource    game_option.resource_dist = args.resource_dist    game_option.fair = args.fair    game_option.save_replay_freq = args.save_replay_freq    game_option.save_replay_per_games = args.save_replay_per_games    game_option.lua_files = args.lua_files    game_option.num_games_per_thread = args.game_per_thread    # !!! this is important    game_option.max_num_units_per_player = args.max_num_units    save_dir = os.path.abspath(args.save_dir)    if not os.path.exists(save_dir):        os.makedirs(save_dir)    game_option.save_replay_prefix = save_dir + '/'    return game_optiondef get_ai_options(args, num_instructions):    options = []    for i in range(2):        ai_option = minirts.AIOption()        ai_option.t_len = 1        ai_option.fs = args.frame_skip        ai_option.fow = args.fow        ai_option.use_moving_avg = args.use_moving_avg        ai_option.moving_avg_decay = args.moving_avg_decay        ai_option.num_resource_bins = args.num_resource_bins        ai_option.resource_bin_size = args.resource_bin_size        ai_option.max_num_units = args.max_num_units        ai_option.num_prev_cmds = args.num_prev_cmds        ai_option.num_instructions = num_instructions[i]        ai_option.max_raw_chars = args.max_raw_chars        ai_option.verbose = args.verbose        options.append(ai_option)    return options[0], options[1]def load_model(coach_path, model_path, args):    if 'onehot' in coach_path:        coach = ConvOneHotCoach.load(coach_path).to(device)    elif 'gen' in coach_path:        coach = RnnGenerator.load(coach_path).to(device)    else:        coach = ConvRnnCoach.load(coach_path).to(device)    coach.max_raw_chars = args.max_raw_chars    executor = Executor.load(model_path).to(device)    executor_wrapper = ExecutorWrapper(        coach, executor, coach.num_instructions, args.max_raw_chars, args.cheat, args.inst_mode)    executor_wrapper.train(False)    return executor_wrapperdef sample(lim=0.2):    rnd = random.uniform(0.0, 1.0)    return rnd <= limdef merge_batch(batch_dict_item, index, batch):    for k, v in batch.items():        if k == "actor":            continue        if k in batch_dict_item:            batch_dict_item[k] = torch.cat((batch_dict_item[k], v[index].unsqueeze(0)), 0)        else:            batch_dict_item[k] = v[index].unsqueeze(0)    return batch_dict_itemdef get_train_batches(p1_win_dict, state_table, iteration):    win_batch_dict = defaultdict(dict)    loss_batch_dict = defaultdict(dict)    for b in state_table:        game_ids = b["game_id"].cpu().numpy()        for i, g_id_n in enumerate(game_ids):            ##Need to convert g_ids from tensor to int            g_id = g_id_n[0]            full_g_id = str(iteration) + "_" + str(g_id)            if p1_win_dict[full_g_id] == 1:                win_batch_dict[g_id].update(merge_batch(win_batch_dict[g_id], i, b))            if p1_win_dict[full_g_id] == -1:                loss_batch_dict[g_id].update(merge_batch(loss_batch_dict[g_id], i, b))    return win_batch_dict, loss_batch_dictdef run_eval(args, model1, model2):    num_eval_games = 100    pbar = tqdm(total=num_eval_games * 2)    result1 = ResultStat('reward', None)    result2 = ResultStat('reward', None)    game_option = get_game_option(args)    ai1_option, ai2_option = get_ai_options(        args, [model1.coach.num_instructions, model2.coach.num_instructions])    context, act1_dc, act2_dc = init_games(        num_eval_games, ai1_option, ai2_option, game_option)    context.start()    dc = DataChannelManager([act1_dc, act2_dc])    i = 0    model1.eval()    model2.eval()    while not context.terminated():        i += 1        # if i % 1000 == 0:        #     print('%d, progress agent1: win %d, loss %d' % (i, result1.win, result1.loss))        data = dc.get_input(max_timeout_s=1)        if len(data) == 0:            continue        for key in data:            # print(key)            batch = to_device(data[key], device)            if key == 'act1':                batch['actor'] = 'act1'                ## Add batches to state table using sampling before adding                ## Add based on the game_id                result1.feed(batch)                with torch.no_grad():                    reply, _ = model1.forward(batch)            elif key == 'act2':                batch['actor'] = 'act2'                result2.feed(batch)                with torch.no_grad():                    reply, _ = model2.forward(batch)            else:                assert False            dc.set_reply(key, reply)            game_ids = batch['game_id'].cpu().numpy()            terminals = batch['terminal'].cpu().numpy().flatten()            for i, g_id in enumerate(game_ids):                if terminals[i] == 1:                    pbar.update(1)    model1.train()    model2.train()    return result1, result2def train_sp_coach(win_batches, loss_batches, model, optimizer, args):    model.train()    optimizer.zero_grad()    mse = torch.nn.MSELoss(reduction='none')    denom = np.sum([y['inst'].size()[0] for x, y in win_batches.items()]) + \            np.sum([y['inst'].size()[0] for x, y in loss_batches.items()])    l1_loss_mean = 0    mse_loss_mean = 0    total_value = 0    for (kind, r), batches in zip(reward_tuple, [win_batches, loss_batches]):        # if kind == "win":        #     denom = np.sum([y['inst'].size()[0] for x, y in win_batches.items()])        # else:        #     denom = np.sum([y['inst'].size()[0] for x, y in loss_batches.items()])        for game_id, batch in batches.items():            log_prob, value = model.get_coach_rl_train_loss(batch)            l1 = (r*torch.ones_like(value) - value.detach())*log_prob            l2 = mse(value, r*torch.ones_like(value))            l1_mean = -1.0*(l1.sum())/denom            mse_loss = 0.1*(l2.sum())/denom            policy_loss = l1_mean + mse_loss            policy_loss.backward()            l1_loss_mean+=l1_mean.item()            mse_loss_mean+= mse_loss.item()            total_value+=value.sum().item()/denom        torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)        optimizer.step()        optimizer.zero_grad()    model.eval()    return l1_loss_mean, mse_loss_mean, total_valuedef self_play():    global device    args = parse_args()    print('args:')    pprint.pprint(vars(args))    os.environ['LUA_PATH'] = os.path.join(args.lua_files, '?.lua')    print('lua path:', os.environ['LUA_PATH'])    if not os.path.exists(args.save_dir):        os.makedirs(args.save_dir)    # writer = SummaryWriter(comment="sp_fixed_c1_type={}_c2_type={}__e1_type={}_e2_type={}__lr={}__num_iter={}__num_thread={}".    #                        format(args.coach1, args.coach2, args.executor1, args.executor2, args.lr, args.num_iter, args.num_thread))    if args.reload:        print("Reloading coach model.... ")        args.coach1 = args.load_file        log_name = "replay_adaptive_c1_type={}__e1_type={}_lr={}__num_iter={}__num_thread={}__sp_factor={}_{}".\                    format(os.path.basename(args.coach1).replace(".pt", ""),                   args.executor1, args.lr, args.num_iter, args.num_thread, args.sp_factor,                           random.randint(1111, 9999))        writer = SummaryWriter(            comment=log_name)    else:        log_name = "sp_adaptive_c1_type={}__e1_type={}_lr={}__num_iter={}__num_thread={}__sp_factor={}_{}".\            format(args.coach1, args.executor1, args.lr, args.num_iter, args.num_thread, args.sp_factor,                   random.randint(1111, 9999))        writer = SummaryWriter(            comment=log_name)        args.coach1 = best_coaches[args.coach1]    args.executor1 = best_executors[args.executor1]    args.coach2 = best_coaches[args.coach2]    args.executor2 = best_executors[args.executor2]    logger_path = os.path.join(args.save_dir, 'train.log')    sys.stdout = Logger(logger_path)    device = torch.device('cuda:%d' % args.gpu)    sp_model = load_model(args.coach1, args.executor1, args)    bc_model = load_model(args.coach2, args.executor2, args)    optimizer = optim.Adam(sp_model.coach.parameters(), lr=args.lr)    best_test_win_pct = 0    print("Progress: ")    ## Create Save folder:    save_folder = os.path.join(args.save_folder, log_name)    if os.path.exists(save_folder):        print("Attempting to create an existing folder..")        import pdb        pdb.set_trace()    os.makedirs(save_folder)    for index in range(args.num_iter):        if index%args.eval_factor == 0:            print("Evaluating model....")            e_result1, e_result2 = run_eval(args, sp_model, bc_model)            test_win_pct = e_result1.win / e_result1.num_games            writer.add_scalar('Test/Agent-1/Win', e_result1.win / e_result1.num_games, index)            writer.add_scalar('Test/Agent-1/Loss', e_result1.loss / e_result1.num_games, index)            writer.add_scalar('Test/Eval_model/Win', e_result2.win / e_result2.num_games, index)            writer.add_scalar('Test/Eval_model/Loss', e_result2.loss / e_result2.num_games, index)            if test_win_pct > best_test_win_pct:                best_test_win_pct = test_win_pct                model_file = os.path.join(save_folder, 'best_coach_checkpoint_%d.pt' % index)                print('Saving model coach to: ', model_file)                sp_model.coach.save(model_file)        if index % args.sp_factor == 0:            print("Playing against itself")            model1 = sp_model            model2 = sp_model        else:            print("Playing against BC model")            model1 = sp_model            model2 = bc_model        game_option = get_game_option(args)        ai1_option, ai2_option = get_ai_options(            args, [model1.coach.num_instructions, model2.coach.num_instructions])        ## Launching games        context, act1_dc, act2_dc = init_games(            args.num_thread, ai1_option, ai2_option, game_option)        context.start()        dc = DataChannelManager([act1_dc, act2_dc])        i = 0        result1 = ResultStat('reward', None)        result2 = ResultStat('reward', None)        state_table = []        p1dict = defaultdict(list)        p2dict = defaultdict(list)        p1_win_dict = {}        p2_win_dict = {}        pbar = tqdm(total=args.num_thread * 2)        model1.train()        model2.train()        while not context.terminated():            i += 1            data = dc.get_input(max_timeout_s=1)            if len(data) == 0:                continue            for key in data:                # print(key)                batch = to_device(data[key], device)                if key == 'act1':                    batch['actor'] = 'act1'                    ## Add batches to state table using sampling before adding                    ## Add based on the game_id                    result1.feed(batch)                    with torch.no_grad():                        reply, log_prob_reply = model1.forward(batch)                    if sample(lim=args.sampling_freq):                        batch.update(log_prob_reply['samples'])                        state_table.append(batch)                elif key == 'act2':                    batch['actor'] = 'act2'                    result2.feed(batch)                    with torch.no_grad():                        reply, log_prob_reply = model2.forward(batch)                else:                    assert False                dc.set_reply(key, reply)                inst = reply['inst']  ## Get get_inst                game_ids = batch['game_id'].cpu().numpy()                terminals = batch['terminal'].cpu().numpy().flatten()                rewards = batch['reward'].cpu().numpy().flatten()                for i, g_id in enumerate(game_ids):                    act_dict = p1dict if key == 'act1' else p2dict                    act_model = sp_model if key == 'act1' else model2                    act_win_dict = p1_win_dict if key == 'act1' else p2_win_dict                    dict_key = str(index) + "_" + str(g_id[0])                    act_dict[dict_key].append(act_model.coach.inst_dict.get_inst(inst[i]))                    if terminals[i] == 1:                        pbar.update(1)                        # print("Game {} has terminated.".format(g_id[0]))                        if rewards[i] == 1:                            act_win_dict[dict_key] = 1                        elif rewards[i] == -1:                            act_win_dict[dict_key] = -1        dc.terminate()        win_batches, loss_batches = get_train_batches(p1_win_dict, state_table, index)        l1, mse_loss, value = train_sp_coach(win_batches, loss_batches, sp_model, optimizer, args)        del state_table        del win_batches        del loss_batches        print(result1.log(0))        print(result2.log(0))        writer.add_scalar('Loss/RL-Loss', l1, index)        writer.add_scalar('Loss/MSE-Loss', mse_loss, index)        writer.add_scalar('Value', value, index)        writer.add_scalar('Train/Agent-1/Win', result1.win / result1.num_games, index)        writer.add_scalar('Train/Agent-1/Loss', result1.loss / result1.num_games, index)        writer.add_scalar('Train/Agent-2/Win', result2.win / result2.num_games, index)        writer.add_scalar('Train/Agent-2/Loss', result2.loss / result2.num_games, index)        pbar.close()    writer.close()if __name__ == '__main__':    self_play()