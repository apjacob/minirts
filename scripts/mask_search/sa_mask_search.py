# Copyright (c) Facebook, Inc. and its affiliates.# All rights reserved.## This source code is licensed under the license found in the# LICENSE file in the root directory of this source tree.#import argparseimport osimport sysimport pprintfrom set_path import append_sys_pathappend_sys_path()import torchimport tubefrom pytube import DataChannelManagerimport minirtsimport numpy as npimport randomimport timefrom torch.utils.tensorboard import SummaryWriterfrom simanneal.anneal import time_stringimport picklefrom collections import defaultdictfrom rnn_coach import ConvRnnCoachfrom onehot_coach import ConvOneHotCoachfrom rnn_generator import RnnGeneratorfrom itertools import groupbyfrom executor_wrapper import ExecutorWrapperfrom executor import Executorfrom common_utils import to_device, ResultStat, Loggerfrom best_models import best_executors, best_coachesfrom tqdm import tqdmfrom simanneal import Annealerp1dict = defaultdict(list)p2dict = defaultdict(list)p1_win_dict = {}p2_win_dict = {}def maskify(state, num_instr):    np_mask = np.zeros(num_instr)    np_mask[state] = 1    return np_maskclass MaskSearchAnnealer(Annealer):    """Annealer for the mask search problem    - State: initial mask    """    def __init__(self, mask, args, device, swaps, model1, model2, writer):        self.args = args        self.device = device        self.model1 = model1        self.model2 = model2        self.num_swaps = swaps        self.result1 = ResultStat('reward', None)        self.result2 = ResultStat('reward', None)        self.writer = writer        super(MaskSearchAnnealer, self).__init__(mask)  # important!    def move(self):        """Swaps self.swap number of mask entries from the state mask."""        self.state = swap_mask(self.state, self.num_swaps,                               self.model1.coach.num_instructions, self.args.mask_size)    def energy(self):        """Calculates the energy."""        args = self.args        device = self.device        model1 = self.model1        model2 = self.model2        # for index in range(args.num_iter):        self.result1 = ResultStat('reward', None)        self.result2 = ResultStat('reward', None)        game_option = get_game_option(args)        ai1_option, ai2_option = get_ai_options(            args, [model1.coach.num_instructions, model2.coach.num_instructions])        context, act1_dc, act2_dc = init_games(            args.num_thread, ai1_option, ai2_option, game_option)        context.start()        dc = DataChannelManager([act1_dc, act2_dc])        cur_agent_mask = maskify(self.state, model1.coach.num_instructions)        # print("Simulating random agent {} with mask count of {} against {} agent".format(index, args.mask_size,        #                                                                                  coach2_type))        pbar = tqdm(total=args.num_thread * 2)        i = 0        while not context.terminated():            i += 1            # if i % 1000 == 0:            #     print('%d, progress agent1: win %d, loss %d' % (i, result1.win, result1.loss))            data = dc.get_input(max_timeout_s=1)            if len(data) == 0:                continue            for key in data:                # print(key)                batch = to_device(data[key], device)                if key == 'act1':                    batch['actor'] = 'act1'                    self.result1.feed(batch)                    with torch.no_grad():                        reply = model1.forward(batch, agent_mask=cur_agent_mask)                elif key == 'act2':                    batch['actor'] = 'act2'                    self.result2.feed(batch)                    with torch.no_grad():                        reply = model2.forward(batch)                else:                    assert False                dc.set_reply(key, reply)                game_ids = batch['game_id'].cpu().numpy()                terminals = batch['terminal'].cpu().numpy().flatten()                for i, g_id in enumerate(game_ids):                    if terminals[i] == 1:                        pbar.update(1)        dc.terminate()        energy = self.result1.loss/self.result1.num_games - self.result1.win/self.result1.num_games \                 - self.result2.loss/self.result2.num_games + self.result2.win/self.result2.num_games        pbar.close()        # print("#####################################################")        # print(result1.log(0))        # print(result2.log(0))        # print("Energy: ", energy)        # print("#####################################################")        return energy    def update(self, step, T, E, acceptance, improvement):        elapsed = time.time() - self.start        if step == 0:            print(' Temperature        Energy    Accept   Improve     Elapsed   Remaining',                  file=sys.stderr)            print('%12.5f  %12.2f                      %s            ' %                  (T, E, time_string(elapsed)), file=sys.stderr)            print("Mask: ", self.state)            sys.stderr.flush()        else:            remain = (self.steps - step) * (elapsed / step)            print(' Temperature        Energy    Accept   Improve     Elapsed   Remaining',                  file=sys.stderr)            print('%12.5f  %12.2f  %7.2f%%  %7.2f%%  %s  %s' %                  (T, E, 100.0 * acceptance, 100.0 * improvement,                   time_string(elapsed), time_string(remain)), file=sys.stderr)            print("Mask: ", self.state)            writer.add_scalar('Stats/Accept', 100.0 * acceptance, step)            writer.add_scalar('Stats/Improve', 100.0 * improvement, step)            sys.stderr.flush()        writer.add_scalar('Temperature', T, step)        writer.add_scalar('Agent-1/Win', self.result1.win/self.result1.num_games, step)        writer.add_scalar('Agent-1/Loss', self.result1.loss/self.result1.num_games, step)        writer.add_scalar('Agent-2/Win', self.result2.win / self.result2.num_games, step)        writer.add_scalar('Agent-2/Loss', self.result2.loss / self.result2.num_games, step)        writer.add_scalar('Energy', E, step)        writer.add_text('Mask', str(self.state), step)def init_games(num_games, ai1_option, ai2_option, game_option, *, act_name='act'):    batchsize = min(32, max(num_games // 2, 1))    act1_dc = tube.DataChannel(act_name+'1', batchsize, 1)    act2_dc = tube.DataChannel(act_name+'2', batchsize, 1)    context = tube.Context()    idx2utype = [        minirts.UnitType.SPEARMAN,        minirts.UnitType.SWORDMAN,        minirts.UnitType.CAVALRY,        minirts.UnitType.DRAGON,        minirts.UnitType.ARCHER,    ]    for i in range(num_games):        g_option = minirts.RTSGameOption(game_option)        g_option.seed = game_option.seed + i        g_option.game_id = str(i)        if game_option.save_replay_prefix:            g_option.save_replay_prefix = game_option.save_replay_prefix + "_0_" + str(i)        g = minirts.RTSGame(g_option)        bot1 = minirts.CheatExecutorAI(ai1_option, 0, None, act1_dc)        bot2 = minirts.CheatExecutorAI(ai2_option, 0, None, act2_dc)        # utype = idx2utype[i % len(idx2utype)]        # bot2 = minirts.MediumAI(ai2_option, 0, None, utype, False)        # p1dict[i] = []        # p2dict[i] = []        g.add_bot(bot1)        g.add_bot(bot2)        context.push_env_thread(g)    return context, act1_dc, act2_dcdef parse_args():    parser = argparse.ArgumentParser(description='human coach')    parser.add_argument('--seed', type=int, default=1)    parser.add_argument('--num_thread', type=int, default=1)    parser.add_argument('--num_iter', type=int, default=10)    parser.add_argument('--game_per_thread', type=int, default=1)    parser.add_argument('--gpu', type=int, default=0)    root = os.path.dirname(        os.path.dirname(os.path.dirname(os.path.abspath(__file__))))    default_lua = os.path.join(root, 'game/game_MC/lua')    parser.add_argument('--lua_files', type=str, default=default_lua)    # ai1 option    parser.add_argument('--frame_skip', type=int, default=50)    parser.add_argument('--fow', type=int, default=1)    parser.add_argument('--use_moving_avg', type=int, default=1)    parser.add_argument('--moving_avg_decay', type=float, default=0.98)    parser.add_argument('--num_resource_bins', type=int, default=11)    parser.add_argument('--resource_bin_size', type=int, default=50)    parser.add_argument('--max_num_units', type=int, default=50)    parser.add_argument('--num_prev_cmds', type=int, default=25)    parser.add_argument('--mask_size', type=int, default=50)    parser.add_argument('--swaps', type=int, default=10)    # TOOD: add max instruction span    parser.add_argument('--max_raw_chars', type=int, default=200)    parser.add_argument('--verbose', action='store_true')    parser.add_argument('--inst_mode', type=str, default='custom') # can be full/good/better/custom aka Agent 1 mask    parser.add_argument('--base_inst_mode', type=str, default='full')  # can be full/good/better/custom aka Agent 2 mask    # game option    parser.add_argument('--max_tick', type=int, default=int(2e4))    parser.add_argument('--no_terrain', action='store_true')    parser.add_argument('--resource', type=int, default=500)    parser.add_argument('--resource_dist', type=int, default=4)    parser.add_argument('--fair', type=int, default=0)    parser.add_argument('--save_replay_freq', type=int, default=0)    parser.add_argument('--save_replay_per_games', type=int, default=1)    parser.add_argument('--save_dir', type=str, default='matches2/dev')    # full vision    parser.add_argument('--cheat', type=int, default=0)    parser.add_argument('--coach1', type=str, default='')    # parser.add_argument('--executor1', type=str, default='')    parser.add_argument('--coach2', type=str, default='')    # parser.add_argument('--coach2', type=str, default='')    # parser.add_argument('--executor2', type=str, default='')    args = parser.parse_args()    return argsdef get_game_option(args):    game_option = minirts.RTSGameOption()    game_option.seed = args.seed    game_option.max_tick = args.max_tick    game_option.no_terrain = args.no_terrain    game_option.resource = args.resource    game_option.resource_dist = args.resource_dist    game_option.fair = args.fair    game_option.save_replay_freq = args.save_replay_freq    game_option.save_replay_per_games = args.save_replay_per_games    game_option.lua_files = args.lua_files    game_option.num_games_per_thread = args.game_per_thread    # !!! this is important    game_option.max_num_units_per_player = args.max_num_units    save_dir = os.path.abspath(args.save_dir)    if not os.path.exists(save_dir):        os.makedirs(save_dir)    game_option.save_replay_prefix = save_dir + '/'    return game_optiondef get_ai_options(args, num_instructions):    options = []    for i in range(2):        ai_option = minirts.AIOption()        ai_option.t_len = 1        ai_option.fs = args.frame_skip        ai_option.fow = args.fow        ai_option.use_moving_avg = args.use_moving_avg        ai_option.moving_avg_decay = args.moving_avg_decay        ai_option.num_resource_bins = args.num_resource_bins        ai_option.resource_bin_size = args.resource_bin_size        ai_option.max_num_units = args.max_num_units        ai_option.num_prev_cmds = args.num_prev_cmds        ai_option.num_instructions = num_instructions[i]        ai_option.max_raw_chars = args.max_raw_chars        ai_option.verbose = args.verbose        options.append(ai_option)    return options[0], options[1]def load_model(coach_path, model_path, args, agent):    if 'onehot' in coach_path:        coach = ConvOneHotCoach.load(coach_path).to(device)    elif 'gen' in coach_path:        coach = RnnGenerator.load(coach_path).to(device)    else:        coach = ConvRnnCoach.load(coach_path).to(device)    coach.max_raw_chars = args.max_raw_chars    executor = Executor.load(model_path).to(device)    if agent == 1:        inst_mode = args.inst_mode    else:        inst_mode = args.base_inst_mode    executor_wrapper = ExecutorWrapper(        coach, executor, coach.num_instructions, args.max_raw_chars, args.cheat, inst_mode)    executor_wrapper.train(False)    return executor_wrapperdef get_init_mask(num_instr, mask_size):    mask = random.sample(range(0, num_instr), mask_size)    return maskdef swap_mask(cur_mask, num_swaps, num_instr, mask_size):    full_set = set(range(0, num_instr))    cur_mask_set = set(cur_mask)    diff_set = full_set.difference(cur_mask_set)    swap_list = random.sample(list(diff_set), num_swaps)    new_mask_list =  random.sample(list(cur_mask_set), mask_size - num_swaps)    return swap_list + new_mask_listif __name__ == '__main__':    args = parse_args()    print('args:')    pprint.pprint(vars(args))    os.environ['LUA_PATH'] = os.path.join(args.lua_files, '?.lua')    print('lua path:', os.environ['LUA_PATH'])    if not os.path.exists(args.save_dir):        os.makedirs(args.save_dir)    logger_path = os.path.join(args.save_dir, 'train.log')    sys.stdout = Logger(logger_path)    device = torch.device('cuda:%d' % args.gpu)    writer = SummaryWriter(comment="__a1_type={}__a2_type={}__mask-size={}__swaps={}__num_iter={}__num_thread={}".                           format(args.coach1, args.coach2, args.mask_size, args.swaps, args.num_iter, args.num_thread))    # Training model    args.coach1 = best_coaches[args.coach1]    args.executor1 = best_executors["rnn"]    model1 = load_model(args.coach1, args.executor1, args, 1)    # Baseline model (Model 2 is fixed)    ## Agent 2 is the fixed agent using just the top 50 instructions    coach2_type = args.coach2    args.coach2 = best_coaches[args.coach2]    args.executor2 = best_executors["rnn"]    model2 = load_model(args.coach2, args.executor2, args, 2)    init_mask = get_init_mask(num_instr=model1.coach.num_instructions, mask_size=args.mask_size)    sa_rts = MaskSearchAnnealer(init_mask, args, device, args.swaps, model1, model2, writer)    sa_rts.Tmax = 1.5    sa_rts.Tmin = 0.05    sa_rts.copy_strategy = "slice"    sa_rts.updates = args.num_iter    sa_rts.steps = args.num_iter    state, e = sa_rts.anneal()    print("State mask: ", state)    print("Energy: ", e)    writer.close()